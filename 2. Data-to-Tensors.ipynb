{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding:utf-8 -*-\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "SEED = 12345\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load preprocessed the seneor label data and response data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('train_index', 'rb') as f:\n",
    "    train_index = pickle.load(f)\n",
    "    \n",
    "with open ('test_index', 'rb') as f:\n",
    "    test_index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('label_list.pkl', 'rb') as f:\n",
    "    label_list = pickle.load(f)\n",
    "\n",
    "# with open ('splits/label_train_list.pkl', 'rb') as f:\n",
    "#     label_train_list = pickle.load(f)\n",
    "    \n",
    "# with open ('splits/label_test_list.pkl', 'rb') as f:\n",
    "#     label_test_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('data_list.pkl', 'rb') as f:\n",
    "    data_list = pickle.load(f)\n",
    "    \n",
    "# with open ('splits/data_train_list.pkl', 'rb') as f:\n",
    "#     data_train_list = pickle.load(f)\n",
    "    \n",
    "# with open ('splits/data_test_list.pkl', 'rb') as f:\n",
    "#     data_test_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using train data, estimate the distribution for each sensor\n",
    "Load the best KDE model for each sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_data_all = np.concatenate(data_list[train_index])\n",
    "sensor_data_all = sensor_data_all[:,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(424710, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor_data_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([518.0, 427.0, 795.0, 970.0, 979.0, 1204.0, 1110.0, 1191.0],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_max = np.max(sensor_data_all, axis=0)\n",
    "true_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1204.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESPONSE_MAX = np.max(true_max)\n",
    "RESPONSE_MAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_labels = np.sort(label_list['Class_Label'].unique())\n",
    "class_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the raw data into image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_X = 256\n",
    "IMAGE_Y = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sensor_data_labeled:\n",
    "    def __init__(self, idx, label, data, split):\n",
    "        self.idx = idx\n",
    "        self.label = label\n",
    "        self.data = data\n",
    "        self.split = split\n",
    "        \n",
    "    def convert_datetime(self, date_time_str):\n",
    "        return datetime.datetime.strptime(date_time_str, '%Y-%m-%d %H:%M:%S').timestamp()\n",
    "    \n",
    "    def sample_transformation(self, IMAGE_X, IMAGE_Y, RESPONSE_MAX, NOISE):\n",
    "        data = self.data\n",
    "        x = data[:,0]\n",
    "        #x = [self.convert_datetime(item) for item in x]\n",
    "        x = np.array(range(len(x)))/10\n",
    "\n",
    "        temp = data[:,1]\n",
    "        x_new = np.linspace(0, max(x), 2**7, endpoint=True)\n",
    "        f_interpolation = interp1d(x, temp, kind='linear')\n",
    "        temp_new = f_interpolation(x_new)\n",
    "        temp_tensor = torch.tensor(temp_new.astype(np.float))\n",
    "\n",
    "        humi = data[:,2]\n",
    "        x_new = np.linspace(0, max(x), 2**7, endpoint=True)\n",
    "        f_interpolation = interp1d(x, humi, kind='linear')\n",
    "        humi_new = f_interpolation(x_new)\n",
    "        humi_tensor = torch.tensor(humi_new.astype(np.float))\n",
    "\n",
    "        x_2 = x[np.arange(int(len(x)/10))*10]\n",
    "        res_list = []\n",
    "        for i in range(3,data.shape[1]):\n",
    "            y = data[:,i]\n",
    "            \n",
    "            # Denoise using moving average\n",
    "            y_series = pd.Series(y)\n",
    "            y_2 = y_series.rolling(10).mean()\n",
    "            y_2 = y_2.fillna(y_2.dropna().iloc[0]).to_numpy()\n",
    "            x = x_2\n",
    "            y = y_2[np.arange(int(len(y_2)/10))*10]\n",
    "            \n",
    "            x_new = np.linspace(0, max(x), 2**14, endpoint=True)\n",
    "            f_interpolation = interp1d(x, y, kind='linear')\n",
    "            y_new = f_interpolation(x_new)\n",
    "            y_new = np.multiply(y_new, [np.random.normal(1, NOISE)] * len(y_new))\n",
    "            y_new = np.array([y if y < RESPONSE_MAX else RESPONSE_MAX for y in y_new])\n",
    "\n",
    "            #2d hist\n",
    "            x_edge = np.linspace(0, max(x_new), IMAGE_X+1, endpoint=True)\n",
    "            y_edge = np.linspace(0, RESPONSE_MAX, IMAGE_Y+1, endpoint=True)\n",
    "            H, xedges, yedges = np.histogram2d(x_new, y_new, bins=(x_edge, y_edge))\n",
    "            H = H.T\n",
    "            H_flip = np.flipud(H)\n",
    "            #H_filp_normal = (H_flip - np.min(H_flip))/(np.max(H_flip)-np.min(H_flip))*255\n",
    "\n",
    "            res = cv2.resize(H_flip, dsize=(IMAGE_X, IMAGE_Y), interpolation=cv2.INTER_AREA)\n",
    "            res_list.append(res.astype(np.uint8))\n",
    "        res_tensor = torch.tensor(res_list)\n",
    "\n",
    "        return [temp_tensor, humi_tensor, res_tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_index(label_list, class_labels, train_index, num_ratio = 0.25):\n",
    "    valid_index = []\n",
    "    label_train_list = label_list.loc[train_index]\n",
    "    for idx in class_labels:\n",
    "        label_idx_list = label_train_list.loc[label_train_list['Class_Label']==idx].index.to_numpy()\n",
    "        label_idx_valid = np.random.choice(label_idx_list, int(round(len(label_idx_list)*num_ratio)), replace=False)\n",
    "        valid_index.append(label_idx_valid)\n",
    "    valid_index = np.concatenate(valid_index)\n",
    "    return valid_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_train_index(label_list, class_labels, train_index, num_total=1000):\n",
    "    num_resample = num_total/len(class_labels)\n",
    "    index_resample = []\n",
    "    label_train_list = label_list.loc[train_index]\n",
    "    for idx in class_labels:\n",
    "        label_idx_list = label_train_list.loc[label_train_list['Class_Label']==idx].index.to_numpy()\n",
    "        label_idx_resample = np.random.choice(label_idx_list, int(num_resample), replace=True)\n",
    "        index_resample.append(label_idx_resample)\n",
    "    index_resample = np.concatenate(index_resample)\n",
    "    return index_resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_index = get_valid_index(label_list, class_labels, train_index, num_ratio = 0.25)\n",
    "train_index_selected = [ii for ii in train_index if ii not in valid_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all data counts for class:\t\t[54 36 18 54 18]\n",
      "all data counts for class(unique):\t[54 36 18 54 18]\n",
      "train data counts for class:\t\t[32 22 10 32 10]\n",
      "valid data counts for class:\t\t[11  7  4 11  4]\n",
      "test data counts for class:\t\t[11  7  4 11  4]\n"
     ]
    }
   ],
   "source": [
    "all_index = np.concatenate([train_index_selected, valid_index, test_index])\n",
    "all_data_counts = label_list.loc[all_index]['Class_Label'].value_counts().sort_index().to_numpy()\n",
    "all_data_counts_unique = label_list.loc[np.unique(all_index)]['Class_Label'].value_counts().sort_index().to_numpy()\n",
    "\n",
    "print('all data counts for class:\\t\\t{}'.format(all_data_counts))\n",
    "print('all data counts for class(unique):\\t{}'.format(all_data_counts_unique))\n",
    "\n",
    "train_data_counts = label_list.loc[train_index_selected]['Class_Label'].value_counts().sort_index().to_numpy()\n",
    "valid_data_counts = label_list.loc[valid_index]['Class_Label'].value_counts().sort_index().to_numpy()\n",
    "test_data_counts = label_list.loc[test_index]['Class_Label'].value_counts().sort_index().to_numpy()\n",
    "\n",
    "print('train data counts for class:\\t\\t{}'.format(train_data_counts))\n",
    "print('valid data counts for class:\\t\\t{}'.format(valid_data_counts))\n",
    "print('test data counts for class:\\t\\t{}'.format(test_data_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train resample data counts for class:\t[200 200 200 200 200]\n"
     ]
    }
   ],
   "source": [
    "train_index_resample = resample_train_index(label_list, class_labels, train_index_selected, num_total=1000)\n",
    "train_resample_data_counts = label_list.loc[train_index_resample]['Class_Label'].value_counts().sort_index().to_numpy()\n",
    "print('train resample data counts for class:\\t{}'.format(train_resample_data_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -rf tensor_resamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1074/1074 [02:04<00:00,  8.64it/s]\n",
      "100%|██████████| 1074/1074 [02:06<00:00,  8.51it/s]\n",
      "100%|██████████| 1074/1074 [02:05<00:00,  8.57it/s]\n",
      "100%|██████████| 1074/1074 [01:58<00:00,  9.07it/s]\n",
      "  0%|          | 0/1074 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttensor_resamples/CV_0 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1074/1074 [02:03<00:00,  8.71it/s]\n",
      "100%|██████████| 1074/1074 [02:06<00:00,  8.50it/s]\n",
      "100%|██████████| 1074/1074 [02:00<00:00,  8.94it/s]\n",
      "100%|██████████| 1074/1074 [02:04<00:00,  8.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttensor_resamples/CV_1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1074/1074 [01:52<00:00,  9.58it/s]\n",
      "100%|██████████| 1074/1074 [02:04<00:00,  8.63it/s]\n",
      "100%|██████████| 1074/1074 [02:33<00:00,  7.01it/s]\n",
      "100%|██████████| 1074/1074 [02:27<00:00,  7.30it/s]\n",
      "  0%|          | 0/1074 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttensor_resamples/CV_2 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1074/1074 [02:43<00:00,  6.56it/s]\n",
      "100%|██████████| 1074/1074 [02:30<00:00,  7.12it/s]\n",
      "100%|██████████| 1074/1074 [02:31<00:00,  7.09it/s]\n",
      "100%|██████████| 1074/1074 [02:35<00:00,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttensor_resamples/CV_3 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1074/1074 [02:26<00:00,  7.31it/s]\n",
      "100%|██████████| 1074/1074 [02:32<00:00,  7.05it/s]\n",
      "100%|██████████| 1074/1074 [02:35<00:00,  6.90it/s]\n",
      "100%|██████████| 1074/1074 [02:36<00:00,  6.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttensor_resamples/CV_4 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('tensor_resamples'):\n",
    "    os.mkdir('tensor_resamples')\n",
    "\n",
    "for i in range(5):\n",
    "    cv_path = 'tensor_resamples/CV_{}'.format(i)\n",
    "    if not os.path.exists(cv_path):\n",
    "        os.mkdir(cv_path)\n",
    "    \n",
    "    np.random.shuffle(train_index)\n",
    "\n",
    "    valid_index_selected = get_valid_index(label_list, class_labels, train_index, num_ratio = 0.25)\n",
    "    train_index_selected = [ii for ii in train_index if ii not in valid_index_selected]\n",
    "    test_index_selected = test_index\n",
    "    \n",
    "    train_index_resample = resample_train_index(label_list, class_labels, train_index_selected, num_total=1000)\n",
    "    resample_index = np.concatenate([train_index_resample, valid_index_selected, test_index_selected])\n",
    "\n",
    "    sensor_data_label_list = []\n",
    "    for idx in resample_index:\n",
    "        label = label_list.iloc[idx] # pandas dataframe\n",
    "        data = data_list[idx] # numpy array\n",
    "\n",
    "        if idx in train_index_selected: \n",
    "            split = 'train'\n",
    "        elif idx in valid_index_selected: \n",
    "            split = 'valid'\n",
    "        elif idx in test_index_selected: \n",
    "            split = 'test'\n",
    "        sensor_data_label_list.append(sensor_data_labeled(idx, label, data, split))\n",
    "\n",
    "    for noise in [0.00, 0.01, 0.03, 0.05]:\n",
    "        if not os.path.exists(os.path.join(cv_path, '{:0.2f}'.format(noise))):\n",
    "            os.mkdir(os.path.join(cv_path, '{:0.2f}'.format(noise)))\n",
    "        for split in ['train', 'valid', 'test']:\n",
    "            if not os.path.exists(os.path.join(cv_path, '{:0.2f}'.format(noise), str(split))):\n",
    "                os.mkdir(os.path.join(cv_path, '{:0.2f}'.format(noise), str(split)))\n",
    "            for idx in class_labels:\n",
    "                if not os.path.exists(os.path.join(cv_path, '{:0.2f}'.format(noise), str(split), str(idx))):\n",
    "                    os.mkdir(os.path.join(cv_path, '{:0.2f}'.format(noise), str(split), str(idx)))\n",
    "        cnt = 0\n",
    "        for sensor_data in tqdm(sensor_data_label_list):\n",
    "            split = sensor_data.split\n",
    "            idx = sensor_data.label.idx\n",
    "            class_label = sensor_data.label.Class_Label\n",
    "            data = sensor_data.data\n",
    "            tensor = sensor_data.sample_transformation(IMAGE_X, IMAGE_Y, RESPONSE_MAX, noise)\n",
    "\n",
    "            file_name = 'tensor_{}_{:03d}_{:03d}'.format(class_label, int(idx), cnt)    \n",
    "            file_path = os.path.join(cv_path, '{:0.2f}'.format(noise), str(split), str(class_label), file_name)\n",
    "\n",
    "            torch.save(tensor, file_path)\n",
    "            cnt += 1\n",
    "    print('\\t{} completed'.format(cv_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gas",
   "language": "python",
   "name": "gas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
